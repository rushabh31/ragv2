##############################################
# Ingestion Service Configuration
##############################################

# System-wide settings
system:
  log_level: INFO
  request_id_header: X-Request-ID
  environment: development

# Security settings
security:
  api_keys:
    - name: default
      key: "${API_KEY}"
      roles:
        - user
    - name: admin
      key: "${ADMIN_API_KEY}"
      roles:
        - user
        - admin
  rate_limiting:
    enabled: true
    provider: local
    requests: 100
    period_seconds: 60

# Cache settings
cache:
  enabled: true
  provider: local
  default_ttl_seconds: 3600
  max_size: 10000
  cleanup_interval_seconds: 300

# Ingestion API settings
ingestion:
  port: 8000
  host: 0.0.0.0
  workers: 4
  parsers:
    default_parser: simple_text
    simple_text:
      enabled: true
    pymupdf:
      enabled: false
    vision_parser:
      enabled: false
      model: gemini-1.5-pro-vision
      # project_id and location loaded directly from environment variables
    groq_simplev_vision:
      enabled: true
      model: meta-llama/llama-4-scout-17b-16e-instruct
      api_key: "${GROQ_API_KEY}"
      max_pages: 50
    openai_vision:
      enabled: false
      model: gpt-4-vision-preview
      api_key: "${OPENAI_API_KEY}"
      max_pages: 50
      # For Azure OpenAI, uncomment and configure these
      # api_base: "https://your-resource.openai.azure.com"
      # api_version: "2023-12-01-preview"
      # api_type: "azure"
  chunking:
    strategy: page_based
    # Options for page-based chunking
    min_chunk_size: 100
    use_headers_fallback: true
    # Keep fixed size options for backwards compatibility
    chunk_size: 1000
    chunk_overlap: 200
  embedding:
    # Switch between "vertex", "openai", "sentence_transformer", and "groq"
    provider: sentence_transformer
    # Vertex AI settings - model only
    # project_id and location loaded directly from environment variables
    model: textembedding-gecko-multilingual
    # Sentence Transformer settings
    model_name: all-MiniLM-L6-v2
    batch_size: 32
    max_workers: 1
  vector_store:
    # Choose vector store type: "faiss", "pgvector", or "chromadb"
    type: pgvector
    # Common settings
    # path: ./data/vector_store
    dimension: 384  # Must match embedding model dimension (384 for all-MiniLM-L6-v2)
    
    # FAISS specific settings
    faiss:
      index_type: HNSW     # Options: Flat, HNSW, IVF
      metric: inner_product # Options: inner_product, l2
      nlist: 100           # For IVF indexes
      nprobe: 10           # For IVF search
    
    # PgVector specific settings
    pgvector:
      connection_string: "postgresql://localhost:5432/postgres"
      table_name: "document_embeddings"
      index_method: "hnsw"  # Options: ivfflat, hnsw
    
    # ChromaDB specific settings
    chromadb:
      persist_directory: "./data/chromadb"
      collection_name: "document_embeddings"
      distance_function: "cosine"  # Options: cosine, l2, ip
