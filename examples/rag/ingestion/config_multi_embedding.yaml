# Multi-Provider RAG Ingestion Configuration
# This configuration demonstrates how to use different embedding providers
# with the universal authentication system

# Document Processing Configuration
document_processing:
  chunk_size: 1000
  chunk_overlap: 200
  max_chunks_per_document: 100

# Parser Configuration
parser:
  provider: "groq_vision_parser"
  config:
    model_name: "llama-3.2-11b-vision-preview"
    prompt_template: "Extract and structure the text content from this document."

# Generation Configuration (for semantic chunking and other LLM tasks)
generation:
  provider: "groq"
  config:
    model_name: "llama-3.1-70b-versatile"
    temperature: 0.1
    max_tokens: 2048
    top_p: 0.9

# Embedding Configuration - Choose one provider
embedding:
  # Option 1: Sentence Transformer Embedding (Local, No API Key Required)
  provider: "sentence_transformer"
  config:
    model: "all-MiniLM-L12-v2"
    device: "cpu"  # or "cuda" if GPU available
    normalize_embeddings: true
    batch_size: 32

  # Option 2: Vertex AI Embedding (New Universal Auth)
  # provider: "vertex_ai"
  # config:
  #   model: "text-embedding-004"
  #   project_id: "${PROJECT_ID}"  # From environment
  #   location: "us-central1"
  #   batch_size: 100

  # Option 3: OpenAI Universal Embedding (New Universal Auth)
  # provider: "openai_universal"
  # config:
  #   model: "all-mpnet-base-v2"
  #   base_url: "https://api.openai.com/v1"  # Optional custom base URL
  #   batch_size: 100

  # Option 4: Azure OpenAI Embedding (New Universal Auth)
  # provider: "azure_openai"
  # config:
  #   model: "modelname"
  #   azure_endpoint: "https://your-resource.openai.azure.com"
  #   api_version: "2023-05-15"
  #   batch_size: 100

  # Option 5: Original Vertex Embedding (Legacy)
  # provider: "vertex"
  # config:
  #   model: "textembedding-gecko-multilingual"
  #   batch_size: 100

  # Option 6: Original OpenAI Embedding (Legacy)
  # provider: "openai"
  # config:
  #   model: "text-embedding-ada-002"
  #   batch_size: 100

# Vector Store Configuration
vector_store:
  provider: "faiss"
  config:
    dimension: 384  # Adjust based on embedding model (all-MiniLM-L12-v2 = 384 dims)
    index_type: "HNSW"  # Options: "Flat", "HNSW", "IVF"
    index_path: "./data/faiss_index.bin"
    metadata_path: "./data/faiss_metadata.pkl"

# API Configuration
api:
  host: "0.0.0.0"
  port: 8000
  cors_origins: ["*"]
  api_key_required: true

# Logging Configuration
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"

# Health Check Configuration
health_check:
  enabled: true
  interval_seconds: 30
  timeout_seconds: 10

# Authentication Health Monitoring
auth_monitoring:
  enabled: true
  log_health_status: true
  validate_on_startup: true

# Processing Configuration
processing:
  max_concurrent_documents: 5
  retry_attempts: 3
  retry_delay_seconds: 1
