# Canonical RAG Chatbot Configuration
# Generated from actual code analysis - all parameters verified from source code

# Chatbot Configuration
chatbot:
  # Generation Configuration
  generation:
    provider: "vertex"          # Options: "vertex", "anthropic_vertex", "openai", "azure_openai"
    config:
      model_name: "gemini-1.5-pro-002"  # Model name
      temperature: 0.7                   # Generation temperature (0.0-1.0)
      max_tokens: 2048                   # Maximum tokens in response
      prompt_template: "./templates/rag_prompt.jinja2"  # Path to Jinja2 template
      
      # Alternative: Anthropic Vertex AI
      # provider: "anthropic_vertex"
      # config:
      #   model_name: "claude-3-5-sonnet@20240229"
      #   temperature: 0.7
      #   max_tokens: 2048
      #   region: "us-east5"
      
      # Alternative: OpenAI
      # provider: "openai"
      # config:
      #   model_name: "Meta-Llama-3-70B-Instruct"
      #   base_url: "https://api.openai.com/v1"
      #   temperature: 0.7
      #   max_tokens: 2048
      
      # Alternative: Azure OpenAI
      # provider: "azure_openai"
      # config:
      #   model_name: "GPT4-o"
      #   azure_endpoint: "https://your-resource.openai.azure.com"
      #   api_version: "2023-05-15"
      #   temperature: 0.7
      #   max_tokens: 2048

  # Retrieval Configuration
  retrieval:
    # Vector Store Configuration (must match ingestion config)
    vector_store:
      provider: "faiss"         # Options: "faiss", "pgvector", "chromadb"
      config:
        dimension: 768          # Must match embedding model dimension
        index_path: "./data/faiss_index.bin"
        metadata_path: "./data/faiss_metadata.pkl"
    
    # Embedding Configuration (must match ingestion config)
    embedding:
      provider: "vertex"        # Options: "vertex", "vertex_ai", "openai", "openai_universal", "azure_openai"
      config:
        model: "text-embedding-004"     # Must match ingestion embedding model
        batch_size: 100
    
    # Retrieval Parameters
    top_k: 10                   # Number of documents to retrieve
    similarity_threshold: 0.7   # Minimum similarity threshold

  # Reranking Configuration
  reranking:
    type: "custom"              # Options: "cross_encoder", "custom"
    config:
      # Cross-Encoder Reranker
      # model: "cross-encoder/ms-marco-MiniLM-L-6-v2"
      # top_k: 5
      
      # Custom Reranker (Default)
      top_k: 5                  # Number of documents after reranking
      boost_recent: true        # Boost recent documents
      boost_factor: 1.2         # Boost factor for recent documents

  # Memory Configuration
  memory:
    type: "langgraph_checkpoint"  # Options: "simple", "mem0", "langgraph", "langgraph_checkpoint"
    max_history: 10               # Maximum conversation history length
    
    # LangGraph Checkpoint Memory (Default)
    store_type: "postgres"        # Options: "in_memory", "postgres"
    postgres:
      connection_string: "postgresql://username:password@localhost:5432/langgraph_db"
    
    # Alternative: Simple Memory
    # type: "simple"
    # max_history: 10
    
    # Alternative: Mem0 Memory
    # type: "mem0"
    # collection_name: "chat_history"
    # storage_path: "./data/mem0"
    
    # Alternative: LangGraph Memory
    # type: "langgraph"
    # store_type: "in_memory"
    # embedding_dimensions: 384

  # Workflow Configuration
  workflow:
    use_retrieval: true         # Enable document retrieval
    use_reranking: true         # Enable document reranking
    use_memory: true            # Enable conversation memory
    max_retries: 3              # Maximum workflow retries
    timeout_seconds: 30         # Workflow timeout

# API Configuration
api:
  host: "0.0.0.0"               # API host
  port: 8001                    # API port (different from ingestion)
  cors_origins: ["*"]           # CORS allowed origins
  api_key_required: true        # Whether API key is required

# Cache Configuration
cache:
  enabled: true                 # Enable caching
  ttl_seconds: 3600            # Cache TTL in seconds
  max_size: 1000               # Maximum cache entries

# Logging Configuration
logging:
  level: "INFO"                 # Options: "DEBUG", "INFO", "WARNING", "ERROR"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"

# Health Check Configuration
health_check:
  enabled: true                 # Enable health checks
  interval_seconds: 30          # Health check interval
  timeout_seconds: 10           # Health check timeout

# Authentication Health Monitoring
auth_monitoring:
  enabled: true                 # Enable auth monitoring
  log_health_status: true       # Log auth health status
  validate_on_startup: true     # Validate auth on startup

# Processing Configuration
processing:
  max_concurrent_requests: 10   # Maximum concurrent chat requests
  request_timeout_seconds: 60   # Request timeout
  retry_attempts: 3             # Number of retry attempts
  retry_delay_seconds: 1        # Delay between retries

# Chat History Configuration
chat_history:
  use_chat_history: false       # Enable cross-session chat history by default
  chat_history_days: 7          # Days to look back for chat history
  max_messages_per_session: 100 # Maximum messages per session

# Feedback Configuration
feedback:
  enabled: true                 # Enable feedback collection
  store_feedback: true          # Store feedback in database
  feedback_types: ["thumbs_up", "thumbs_down", "rating", "comment"]

# Session Configuration
sessions:
  session_timeout_minutes: 60   # Session timeout in minutes
  max_sessions_per_user: 10     # Maximum sessions per user
  cleanup_interval_minutes: 30  # Session cleanup interval

# Provider-Specific Environment Variables Required:
# 
# For Vertex AI:
# - COIN_CONSUMER_ENDPOINT_URL
# - COIN_CONSUMER_CLIENT_ID  
# - COIN_CONSUMER_CLIENT_SECRET
# - COIN_CONSUMER_SCOPE
# - PROJECT_ID
# - VERTEXAI_API_ENDPOINT
#
# For OpenAI:
# - OPENAI_API_KEY
#
# For Azure OpenAI:
# - AZURE_OPENAI_API_KEY
# - AZURE_OPENAI_ENDPOINT
#
# For Anthropic:
# - ANTHROPIC_API_KEY
#
# For PostgreSQL:
# - Database connection details in connection_string
